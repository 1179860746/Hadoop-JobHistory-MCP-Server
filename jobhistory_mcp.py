#!/usr/bin/env python3
"""
JobHistory MCP Server - Hadoop MapReduce ä½œä¸šå†å²æŸ¥è¯¢æœåŠ¡

è¯¥ MCP Server å°è£…äº† Hadoop JobHistory Server çš„ REST APIï¼Œ
æä¾›å·¥å…·æ¥æŸ¥è¯¢ MapReduce ä½œä¸šå†å²ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- ä½œä¸šåˆ—è¡¨æŸ¥è¯¢ï¼ˆæ”¯æŒè¿‡æ»¤å’Œåˆ†é¡µï¼‰
- ä½œä¸šè¯¦æƒ…æŸ¥è¯¢
- ä½œä¸šè®¡æ•°å™¨æŸ¥è¯¢
- ä½œä¸šé…ç½®æŸ¥è¯¢
- ä»»åŠ¡åˆ—è¡¨å’Œè¯¦æƒ…æŸ¥è¯¢
- ä»»åŠ¡å°è¯•ä¿¡æ¯æŸ¥è¯¢

ä½¿ç”¨ FastMCP æ¡†æ¶æ„å»ºï¼Œæ”¯æŒ Pydantic v2 è¾“å…¥éªŒè¯ã€‚

ç¯å¢ƒå˜é‡:
    JOBHISTORY_URL: JobHistory Server åœ°å€ï¼Œé»˜è®¤ http://localhost:19888/ws/v1/history
    NODEMANAGER_PORT: NodeManager ç«¯å£ï¼Œç”¨äºè·å–å®¹å™¨æ—¥å¿—ï¼Œé»˜è®¤ 8052
    LOG_LEVEL: æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ INFO
    LOG_FILE: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ ./logs/jobhistory_mcp.log
    LOG_MAX_SIZE: å•ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤ 268435456 (256MB)
    LOG_BACKUP_COUNT: ä¿ç•™çš„æ—¥å¿—æ–‡ä»¶æ•°é‡ï¼Œé»˜è®¤ 5
    LOG_TO_STDERR: æ˜¯å¦è¾“å‡ºåˆ° stderrï¼Œé»˜è®¤ true

ä½œè€…: Winston
ç‰ˆæœ¬: 1.3.0
"""

import json
import os
import re
import sys
import time
import uuid
import logging
import functools
from logging.handlers import RotatingFileHandler
from typing import Optional, List, Dict, Any, Callable
from enum import Enum
from datetime import datetime
from contextvars import ContextVar
from pathlib import Path
from urllib.parse import urlparse

import httpx
from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator
from fastmcp import FastMCP

# ==============================================================================
# æ—¥å¿—é…ç½®
# ==============================================================================

# è¯·æ±‚ ID ä¸Šä¸‹æ–‡å˜é‡ï¼Œç”¨äºå…³è”åŒä¸€è¯·æ±‚çš„æ‰€æœ‰æ—¥å¿—
request_id_var: ContextVar[str] = ContextVar('request_id', default='-')


class RequestIdFilter(logging.Filter):
    """æ—¥å¿—è¿‡æ»¤å™¨ï¼Œä¸ºæ¯æ¡æ—¥å¿—æ·»åŠ è¯·æ±‚ ID"""
    
    def filter(self, record: logging.LogRecord) -> bool:
        record.request_id = request_id_var.get()
        return True


def setup_logging() -> logging.Logger:
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    
    æ”¯æŒæ»šåŠ¨æ—¥å¿—ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œ stderrã€‚
    æ³¨æ„ï¼šMCP stdio æ¨¡å¼ä½¿ç”¨ stdout è¿›è¡Œåè®®é€šä¿¡ï¼Œ
    å› æ­¤æ—¥å¿—åªèƒ½è¾“å‡ºåˆ° stderr æˆ–æ–‡ä»¶ã€‚
    
    Returns:
        logging.Logger: é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨
    """
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    log_level = os.getenv("LOG_LEVEL", "INFO").upper()
    log_file = os.getenv("LOG_FILE", "./logs/jobhistory_mcp.log")
    log_max_size = int(os.getenv("LOG_MAX_SIZE", 268435456))  # 256MB
    log_backup_count = int(os.getenv("LOG_BACKUP_COUNT", 5))
    log_to_stderr = os.getenv("LOG_TO_STDERR", "true").lower() == "true"
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger("jobhistory_mcp")
    logger.setLevel(getattr(logging, log_level, logging.INFO))
    
    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨ï¼ˆé¿å…é‡å¤æ·»åŠ ï¼‰
    logger.handlers.clear()
    
    # æ—¥å¿—æ ¼å¼
    log_format = logging.Formatter(
        fmt='%(asctime)s | %(levelname)-5s | %(request_id)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ·»åŠ è¯·æ±‚ ID è¿‡æ»¤å™¨
    request_id_filter = RequestIdFilter()
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆæ»šåŠ¨æ—¥å¿—ï¼‰
    try:
        log_dir = Path(log_file).parent
        log_dir.mkdir(parents=True, exist_ok=True)
        
        file_handler = RotatingFileHandler(
            filename=log_file,
            maxBytes=log_max_size,
            backupCount=log_backup_count,
            encoding='utf-8'
        )
        file_handler.setFormatter(log_format)
        file_handler.addFilter(request_id_filter)
        logger.addHandler(file_handler)
    except Exception as e:
        # å¦‚æœæ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶ï¼Œè¾“å‡ºè­¦å‘Šåˆ° stderr
        print(f"è­¦å‘Šï¼šæ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶ {log_file}: {e}", file=sys.stderr)
    
    # stderr å¤„ç†å™¨
    if log_to_stderr:
        stderr_handler = logging.StreamHandler(sys.stderr)
        stderr_handler.setFormatter(log_format)
        stderr_handler.addFilter(request_id_filter)
        logger.addHandler(stderr_handler)
    
    return logger


# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logging()

# ==============================================================================
# é…ç½®å¸¸é‡
# ==============================================================================

# JobHistory Server åœ°å€ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
JOBHISTORY_BASE_URL = os.getenv(
    "JOBHISTORY_URL",
    "https://jobhistory.hellobike.cn/ws/v1/history"
)

# NodeManager ç«¯å£ï¼Œç”¨äºè·å–å®¹å™¨æ—¥å¿—
NODEMANAGER_PORT = os.getenv("NODEMANAGER_PORT", "8052")

# HTTP è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = float(os.getenv("REQUEST_TIMEOUT", "30.0"))


def _get_logs_base_url() -> str:
    """
    ä» JOBHISTORY_BASE_URL æ„é€ æ—¥å¿—æœåŠ¡çš„åŸºç¡€ URL
    
    ä¾‹å¦‚:
        è¾“å…¥: http://jobhistory.example.com:19888/ws/v1/history
        è¾“å‡º: http://jobhistory.example.com:19888/jobhistory/logs
    
    Returns:
        æ—¥å¿—æœåŠ¡åŸºç¡€ URL
    """
    parsed = urlparse(JOBHISTORY_BASE_URL)
    return f"{parsed.scheme}://{parsed.netloc}/jobhistory/logs"


# æ—¥å¿—æœåŠ¡åŸºç¡€ URL
LOGS_BASE_URL = _get_logs_base_url()

# å¯åŠ¨æ—¥å¿—
logger.info(f"JobHistory MCP Server åˆå§‹åŒ–")
logger.info(f"JobHistory URL: {JOBHISTORY_BASE_URL}")
logger.info(f"Logs Base URL: {LOGS_BASE_URL}")
logger.info(f"NodeManager Port: {NODEMANAGER_PORT}")
logger.info(f"è¯·æ±‚è¶…æ—¶: {REQUEST_TIMEOUT}s")

# ==============================================================================
# æ—¥å¿—è£…é¥°å™¨
# ==============================================================================


def log_tool_call(func: Callable) -> Callable:
    """
    å·¥å…·è°ƒç”¨æ—¥å¿—è£…é¥°å™¨
    
    è®°å½• MCP å·¥å…·çš„è°ƒç”¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
    - å·¥å…·åç§°å’Œå‚æ•°
    - æ‰§è¡Œæ—¶é—´
    - æˆåŠŸæˆ–å¤±è´¥çŠ¶æ€
    
    Args:
        func: è¢«è£…é¥°çš„å·¥å…·å‡½æ•°
        
    Returns:
        è£…é¥°åçš„å‡½æ•°
    """
    @functools.wraps(func)
    async def wrapper(params=None):
        # ç”Ÿæˆè¯·æ±‚ ID
        req_id = str(uuid.uuid4())[:8]
        request_id_var.set(req_id)
        
        # è®°å½•è¯·æ±‚
        tool_name = func.__name__
        params_str = _safe_serialize_params(params)
        logger.info(f"[TOOL_CALL] {tool_name}, params: {params_str}")
        
        start_time = time.time()
        try:
            # æ‰§è¡Œå·¥å…·å‡½æ•°
            result = await func(params) if params is not None else await func()
            
            # è®°å½•æˆåŠŸå“åº”
            duration_ms = (time.time() - start_time) * 1000
            result_size = len(result) if isinstance(result, str) else 0
            logger.info(f"[TOOL_RSP] success, size: {result_size} bytes, duration: {duration_ms:.2f}ms")
            
            return result
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            duration_ms = (time.time() - start_time) * 1000
            logger.error(f"[TOOL_ERR] {type(e).__name__}: {str(e)}, duration: {duration_ms:.2f}ms")
            raise
    
    return wrapper


def _safe_serialize_params(params) -> str:
    """
    å®‰å…¨åœ°åºåˆ—åŒ–å‚æ•°ç”¨äºæ—¥å¿—è®°å½•
    
    å¯¹æ•æ„Ÿä¿¡æ¯è¿›è¡Œè„±æ•å¤„ç†ï¼Œé™åˆ¶é•¿åº¦é¿å…æ—¥å¿—è¿‡å¤§ã€‚
    
    Args:
        params: Pydantic æ¨¡å‹æˆ–å…¶ä»–å‚æ•°å¯¹è±¡
        
    Returns:
        JSON æ ¼å¼çš„å‚æ•°å­—ç¬¦ä¸²
    """
    if params is None:
        return "{}"
    
    try:
        if hasattr(params, 'model_dump'):
            # Pydantic v2 æ¨¡å‹
            data = params.model_dump()
        elif hasattr(params, 'dict'):
            # Pydantic v1 æ¨¡å‹
            data = params.dict()
        else:
            data = str(params)
            
        # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
        result = json.dumps(data, ensure_ascii=False, default=str)
        
        # é™åˆ¶é•¿åº¦
        if len(result) > 500:
            result = result[:500] + "..."
            
        return result
    except Exception:
        return "<åºåˆ—åŒ–å¤±è´¥>"


# ==============================================================================
# åˆå§‹åŒ– MCP Server
# ==============================================================================

mcp = FastMCP("jobhistory_mcp")

# ==============================================================================
# æšä¸¾ç±»å‹å®šä¹‰
# ==============================================================================


class ResponseFormat(str, Enum):
    """
    å“åº”æ ¼å¼æšä¸¾
    
    - MARKDOWN: äººç±»å¯è¯»çš„ Markdown æ ¼å¼ï¼Œé€‚åˆç›´æ¥å±•ç¤º
    - JSON: æœºå™¨å¯è¯»çš„ JSON æ ¼å¼ï¼Œé€‚åˆç¨‹åºå¤„ç†
    """
    MARKDOWN = "markdown"
    JSON = "json"


class JobState(str, Enum):
    """
    ä½œä¸šçŠ¶æ€æšä¸¾
    
    MapReduce ä½œä¸šçš„ç”Ÿå‘½å‘¨æœŸçŠ¶æ€ï¼š
    - NEW: æ–°å»º
    - INITED: å·²åˆå§‹åŒ–
    - RUNNING: è¿è¡Œä¸­
    - SUCCEEDED: æˆåŠŸå®Œæˆ
    - FAILED: å¤±è´¥
    - KILL_WAIT: ç­‰å¾…ç»ˆæ­¢
    - KILLED: å·²ç»ˆæ­¢
    - ERROR: é”™è¯¯
    """
    NEW = "NEW"
    INITED = "INITED"
    RUNNING = "RUNNING"
    SUCCEEDED = "SUCCEEDED"
    FAILED = "FAILED"
    KILL_WAIT = "KILL_WAIT"
    KILLED = "KILLED"
    ERROR = "ERROR"


class TaskType(str, Enum):
    """
    ä»»åŠ¡ç±»å‹æšä¸¾
    
    MapReduce ä»»åŠ¡ç±»å‹ï¼š
    - MAP: Map ä»»åŠ¡ï¼ˆmï¼‰
    - REDUCE: Reduce ä»»åŠ¡ï¼ˆrï¼‰
    """
    MAP = "m"
    REDUCE = "r"


class TaskState(str, Enum):
    """
    ä»»åŠ¡çŠ¶æ€æšä¸¾
    
    ä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸçŠ¶æ€ï¼š
    - NEW: æ–°å»º
    - SCHEDULED: å·²è°ƒåº¦
    - RUNNING: è¿è¡Œä¸­
    - SUCCEEDED: æˆåŠŸ
    - FAILED: å¤±è´¥
    - KILL_WAIT: ç­‰å¾…ç»ˆæ­¢
    - KILLED: å·²ç»ˆæ­¢
    """
    NEW = "NEW"
    SCHEDULED = "SCHEDULED"
    RUNNING = "RUNNING"
    SUCCEEDED = "SUCCEEDED"
    FAILED = "FAILED"
    KILL_WAIT = "KILL_WAIT"
    KILLED = "KILLED"


class LogType(str, Enum):
    """
    å®¹å™¨æ—¥å¿—ç±»å‹æšä¸¾
    
    æ”¯æŒçš„æ—¥å¿—æ–‡ä»¶ç±»å‹ï¼š
    - STDOUT: æ ‡å‡†è¾“å‡º
    - STDERR: æ ‡å‡†é”™è¯¯
    - SYSLOG: ç³»ç»Ÿæ—¥å¿—
    - SYSLOG_SHUFFLE: Shuffle ç³»ç»Ÿæ—¥å¿—
    - PRELAUNCH_OUT: é¢„å¯åŠ¨è¾“å‡º
    - PRELAUNCH_ERR: é¢„å¯åŠ¨é”™è¯¯
    - CONTAINER_LOCALIZER_SYSLOG: å®¹å™¨æœ¬åœ°åŒ–ç³»ç»Ÿæ—¥å¿—
    """
    STDOUT = "stdout"
    STDERR = "stderr"
    SYSLOG = "syslog"
    SYSLOG_SHUFFLE = "syslog.shuffle"
    PRELAUNCH_OUT = "prelaunch.out"
    PRELAUNCH_ERR = "prelaunch.err"
    CONTAINER_LOCALIZER_SYSLOG = "container-localizer-syslog"


# ==============================================================================
# Pydantic è¾“å…¥æ¨¡å‹å®šä¹‰
# ==============================================================================


class BaseInput(BaseModel):
    """
    æ”¯æŒ JSON å­—ç¬¦ä¸²è¾“å…¥çš„åŸºç¡€è¾“å…¥æ¨¡å‹
    
    ç”¨äºå…¼å®¹æŸäº› MCP å®¢æˆ·ç«¯ï¼ˆå¦‚ Cherry Studioï¼‰çš„å‚æ•°åºåˆ—åŒ– bugï¼Œ
    è¿™äº›å®¢æˆ·ç«¯ä¼šå°†å‚æ•°å¯¹è±¡åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²è€Œä¸æ˜¯ç›´æ¥ä¼ é€’å¯¹è±¡ã€‚
    """
    
    @model_validator(mode='before')
    @classmethod
    def parse_json_string(cls, data):
        """
        åœ¨éªŒè¯ä¹‹å‰é¢„å¤„ç†è¾“å…¥æ•°æ®
        
        å¦‚æœè¾“å…¥æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œå…ˆè§£æä¸ºå­—å…¸ã€‚
        è¿™è§£å†³äº†æŸäº› MCP å®¢æˆ·ç«¯å°†å‚æ•°åŒé‡åºåˆ—åŒ–çš„é—®é¢˜ã€‚
        
        Args:
            data: åŸå§‹è¾“å…¥æ•°æ®
            
        Returns:
            å¤„ç†åçš„æ•°æ®ï¼ˆå­—å…¸æˆ–åŸå§‹æ•°æ®ï¼‰
        """
        if isinstance(data, str):
            try:
                parsed = json.loads(data)
                logger.debug(f"å‚æ•°ä» JSON å­—ç¬¦ä¸²è§£æ: {data[:100]}...")
                return parsed
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œä¿æŒåŸæ ·è®©åç»­éªŒè¯å¤„ç†
                pass
        return data


class ListJobsInput(BaseInput):
    """
    åˆ—å‡ºä½œä¸šçš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_list_jobs å·¥å…·ï¼Œæ”¯æŒå¤šç§è¿‡æ»¤æ¡ä»¶å’Œåˆ†é¡µã€‚
    
    Attributes:
        user: æŒ‰ç”¨æˆ·åè¿‡æ»¤
        state: æŒ‰ä½œä¸šçŠ¶æ€è¿‡æ»¤
        queue: æŒ‰é˜Ÿåˆ—åè¿‡æ»¤
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼ˆ1-100ï¼‰
        started_time_begin: å¼€å§‹æ—¶é—´èŒƒå›´çš„èµ·ç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        started_time_end: å¼€å§‹æ—¶é—´èŒƒå›´çš„ç»ˆç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        finished_time_begin: ç»“æŸæ—¶é—´èŒƒå›´çš„èµ·ç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        finished_time_end: ç»“æŸæ—¶é—´èŒƒå›´çš„ç»ˆç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        response_format: å“åº”æ ¼å¼ï¼ˆmarkdown æˆ– jsonï¼‰
    """
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True
    )

    user: Optional[str] = Field(
        default=None,
        description="æŒ‰ç”¨æˆ·åè¿‡æ»¤ä½œä¸šï¼Œä¾‹å¦‚ 'hadoop'"
    )
    state: Optional[JobState] = Field(
        default=None,
        description="æŒ‰ä½œä¸šçŠ¶æ€è¿‡æ»¤ï¼Œå¯é€‰å€¼: NEW, INITED, RUNNING, SUCCEEDED, FAILED, KILLED"
    )
    queue: Optional[str] = Field(
        default=None,
        description="æŒ‰é˜Ÿåˆ—åè¿‡æ»¤ï¼Œä¾‹å¦‚ 'default'"
    )
    limit: Optional[int] = Field(
        default=20,
        ge=1,
        le=100,
        description="è¿”å›çš„æœ€å¤§ä½œä¸šæ•°é‡ï¼ŒèŒƒå›´ 1-100ï¼Œé»˜è®¤ 20"
    )
    started_time_begin: Optional[int] = Field(
        default=None,
        ge=0,
        description="ä½œä¸šå¼€å§‹æ—¶é—´çš„èµ·ç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ï¼Œç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢"
    )
    started_time_end: Optional[int] = Field(
        default=None,
        ge=0,
        description="ä½œä¸šå¼€å§‹æ—¶é—´çš„ç»ˆç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ï¼Œç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢"
    )
    finished_time_begin: Optional[int] = Field(
        default=None,
        ge=0,
        description="ä½œä¸šç»“æŸæ—¶é—´çš„èµ·ç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ï¼Œç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢"
    )
    finished_time_end: Optional[int] = Field(
        default=None,
        ge=0,
        description="ä½œä¸šç»“æŸæ—¶é—´çš„ç»ˆç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ï¼Œç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢"
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼: 'markdown' äººç±»å¯è¯»æ ¼å¼ï¼Œ'json' æœºå™¨å¯è¯»æ ¼å¼"
    )


class GetJobInput(BaseInput):
    """
    è·å–ä½œä¸šè¯¦æƒ…çš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_get_job å·¥å…·ã€‚
    
    Attributes:
        job_id: MapReduce ä½œä¸š ID
        response_format: å“åº”æ ¼å¼
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šIDï¼Œæ ¼å¼å¦‚ 'job_1326381300833_2_2'",
        min_length=1,
        max_length=100
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )

    @field_validator('job_id')
    @classmethod
    def validate_job_id(cls, v: str) -> str:
        """éªŒè¯ä½œä¸š ID æ ¼å¼"""
        if not v.strip():
            raise ValueError("ä½œä¸š ID ä¸èƒ½ä¸ºç©º")
        return v.strip()


class GetJobCountersInput(BaseInput):
    """
    è·å–ä½œä¸šè®¡æ•°å™¨çš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_get_job_counters å·¥å…·ã€‚
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šID",
        min_length=1
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetJobConfInput(BaseInput):
    """
    è·å–ä½œä¸šé…ç½®çš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_get_job_conf å·¥å…·ã€‚
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šID",
        min_length=1
    )
    filter_key: Optional[str] = Field(
        default=None,
        description="æŒ‰é…ç½®é”®åè¿‡æ»¤ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼Œä¾‹å¦‚ 'mapreduce'"
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetJobAttemptsInput(BaseInput):
    """
    è·å–ä½œä¸šå°è¯•åˆ—è¡¨çš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_get_job_attempts å·¥å…·ã€‚
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šID",
        min_length=1
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class ListTasksInput(BaseInput):
    """
    åˆ—å‡ºä»»åŠ¡çš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_list_tasks å·¥å…·ã€‚
    
    Attributes:
        job_id: æ‰€å±ä½œä¸š ID
        task_type: ä»»åŠ¡ç±»å‹è¿‡æ»¤ï¼ˆMap æˆ– Reduceï¼‰
        response_format: å“åº”æ ¼å¼
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šID",
        min_length=1
    )
    task_type: Optional[TaskType] = Field(
        default=None,
        description="ä»»åŠ¡ç±»å‹: 'm' è¡¨ç¤º Map ä»»åŠ¡ï¼Œ'r' è¡¨ç¤º Reduce ä»»åŠ¡"
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetTaskInput(BaseInput):
    """
    è·å–ä»»åŠ¡è¯¦æƒ…çš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_get_task å·¥å…·ã€‚
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šID",
        min_length=1
    )
    task_id: str = Field(
        ...,
        description="ä»»åŠ¡IDï¼Œæ ¼å¼å¦‚ 'task_1326381300833_2_2_m_0'",
        min_length=1
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetTaskCountersInput(BaseInput):
    """
    è·å–ä»»åŠ¡è®¡æ•°å™¨çš„è¾“å…¥å‚æ•°æ¨¡å‹
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(..., description="ä½œä¸šID", min_length=1)
    task_id: str = Field(..., description="ä»»åŠ¡ID", min_length=1)
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class ListTaskAttemptsInput(BaseInput):
    """
    åˆ—å‡ºä»»åŠ¡å°è¯•çš„è¾“å…¥å‚æ•°æ¨¡å‹
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(..., description="ä½œä¸šID", min_length=1)
    task_id: str = Field(..., description="ä»»åŠ¡ID", min_length=1)
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetTaskAttemptInput(BaseInput):
    """
    è·å–ä»»åŠ¡å°è¯•è¯¦æƒ…çš„è¾“å…¥å‚æ•°æ¨¡å‹
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(..., description="ä½œä¸šID", min_length=1)
    task_id: str = Field(..., description="ä»»åŠ¡ID", min_length=1)
    attempt_id: str = Field(
        ...,
        description="å°è¯•IDï¼Œæ ¼å¼å¦‚ 'attempt_1326381300833_2_2_m_0_0'",
        min_length=1
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetTaskAttemptCountersInput(BaseInput):
    """
    è·å–ä»»åŠ¡å°è¯•è®¡æ•°å™¨çš„è¾“å…¥å‚æ•°æ¨¡å‹
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(..., description="ä½œä¸šID", min_length=1)
    task_id: str = Field(..., description="ä»»åŠ¡ID", min_length=1)
    attempt_id: str = Field(..., description="å°è¯•ID", min_length=1)
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetTaskAttemptLogsInput(BaseInput):
    """
    è·å–ä»»åŠ¡å°è¯•æ—¥å¿—çš„è¾“å…¥å‚æ•°æ¨¡å‹ï¼ˆå®Œæ•´è·å–ï¼‰
    
    ç”¨äº jobhistory_get_task_attempt_logs å·¥å…·ï¼Œè·å–å®Œæ•´çš„æ—¥å¿—å†…å®¹ã€‚
    æ³¨æ„ï¼šå¤§ä»»åŠ¡å¯èƒ½äº§ç”Ÿå¤§é‡æ—¥å¿—ï¼Œå»ºè®®å…ˆä½¿ç”¨ partial å·¥å…·è¯»å–æœ«å°¾å†…å®¹ã€‚
    
    Attributes:
        job_id: ä½œä¸š ID
        task_id: ä»»åŠ¡ ID
        attempt_id: å°è¯• ID
        log_type: æ—¥å¿—ç±»å‹
        response_format: å“åº”æ ¼å¼
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šID",
        min_length=1
    )
    task_id: str = Field(
        ...,
        description="ä»»åŠ¡ID",
        min_length=1
    )
    attempt_id: str = Field(
        ...,
        description="å°è¯•IDï¼Œæ ¼å¼å¦‚ 'attempt_1326381300833_2_2_m_0_0'",
        min_length=1
    )
    log_type: LogType = Field(
        default=LogType.STDOUT,
        description="æ—¥å¿—ç±»å‹: stdout, stderr, syslog, syslog.shuffle, prelaunch.out, prelaunch.err, container-localizer-syslog"
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


class GetTaskAttemptLogsPartialInput(BaseInput):
    """
    éƒ¨åˆ†è¯»å–ä»»åŠ¡å°è¯•æ—¥å¿—çš„è¾“å…¥å‚æ•°æ¨¡å‹
    
    ç”¨äº jobhistory_get_task_attempt_logs_partial å·¥å…·ï¼ŒæŒ‰å­—èŠ‚èŒƒå›´è¯»å–æ—¥å¿—ã€‚
    é€‚ç”¨äºå¤§ä»»åŠ¡æˆ–é•¿æœŸè¿è¡Œä»»åŠ¡çš„æ—¥å¿—åˆ†æï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–å…¨éƒ¨å†…å®¹ã€‚
    
    Attributes:
        job_id: ä½œä¸š ID
        task_id: ä»»åŠ¡ ID
        attempt_id: å°è¯• ID
        log_type: æ—¥å¿—ç±»å‹
        start: èµ·å§‹å­—èŠ‚ä½ç½®ï¼Œè´Ÿæ•°è¡¨ç¤ºä»æœ«å°¾å€’æ•°
        end: ç»“æŸå­—èŠ‚ä½ç½®ï¼Œ0 è¡¨ç¤ºæ–‡ä»¶æœ«å°¾
        response_format: å“åº”æ ¼å¼
        
    Examples:
        - è¯»å–æœ«å°¾ 4KB: start=-4096, end=0
        - è¯»å–å¼€å¤´ 2KB: start=0, end=2048
        - è¯»å–ä¸­é—´éƒ¨åˆ†: start=1024, end=5120
    """
    model_config = ConfigDict(str_strip_whitespace=True)

    job_id: str = Field(
        ...,
        description="ä½œä¸šID",
        min_length=1
    )
    task_id: str = Field(
        ...,
        description="ä»»åŠ¡ID",
        min_length=1
    )
    attempt_id: str = Field(
        ...,
        description="å°è¯•IDï¼Œæ ¼å¼å¦‚ 'attempt_1326381300833_2_2_m_0_0'",
        min_length=1
    )
    log_type: LogType = Field(
        default=LogType.SYSLOG,
        description="æ—¥å¿—ç±»å‹: stdout, stderr, syslog, syslog.shuffle, prelaunch.out, prelaunch.err, container-localizer-syslog"
    )
    start: int = Field(
        default=-4096,
        description="èµ·å§‹å­—èŠ‚ä½ç½®ã€‚æ­£æ•°ä»æ–‡ä»¶å¼€å¤´è®¡ç®—ï¼Œè´Ÿæ•°ä»æ–‡ä»¶æœ«å°¾å€’æ•°ã€‚é»˜è®¤ -4096 è¡¨ç¤ºä»æœ«å°¾å€’æ•° 4KB å¼€å§‹"
    )
    end: int = Field(
        default=0,
        description="ç»“æŸå­—èŠ‚ä½ç½®ã€‚0 è¡¨ç¤ºæ–‡ä»¶æœ«å°¾ï¼Œæ­£æ•°è¡¨ç¤ºå…·ä½“ä½ç½®ã€‚é»˜è®¤ 0 è¡¨ç¤ºè¯»åˆ°æ–‡ä»¶æœ«å°¾"
    )
    response_format: ResponseFormat = Field(
        default=ResponseFormat.MARKDOWN,
        description="è¾“å‡ºæ ¼å¼"
    )


# ==============================================================================
# å·¥å…·å‡½æ•°ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
# ==============================================================================


async def _make_request(endpoint: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    å‘é€ HTTP GET è¯·æ±‚åˆ° JobHistory Server
    
    è¿™æ˜¯æ‰€æœ‰ API è°ƒç”¨çš„åŸºç¡€å‡½æ•°ï¼Œå°è£…äº†ï¼š
    - HTTP å®¢æˆ·ç«¯åˆ›å»ºå’Œç®¡ç†
    - è¯·æ±‚è¶…æ—¶å¤„ç†
    - JSON å“åº”è§£æ
    - è¯·æ±‚å’Œå“åº”æ—¥å¿—è®°å½•
    
    Args:
        endpoint: API ç«¯ç‚¹è·¯å¾„ï¼ˆç›¸å¯¹äº JOBHISTORY_BASE_URLï¼‰
        params: æŸ¥è¯¢å‚æ•°å­—å…¸
        
    Returns:
        è§£æåçš„ JSON å“åº”æ•°æ®
        
    Raises:
        httpx.HTTPStatusError: HTTP é”™è¯¯çŠ¶æ€ç 
        httpx.TimeoutException: è¯·æ±‚è¶…æ—¶
        httpx.ConnectError: è¿æ¥å¤±è´¥
    """
    url = f"{JOBHISTORY_BASE_URL}/{endpoint}"
    params_str = "&".join(f"{k}={v}" for k, v in (params or {}).items())
    full_url = f"{url}?{params_str}" if params_str else url
    
    # è®°å½•è¯·æ±‚
    logger.info(f"[REST_REQ] GET {full_url}")
    
    start_time = time.time()
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                url,
                params=params,
                timeout=REQUEST_TIMEOUT,
                headers={"Accept": "application/json"}
            )
            
            # è®¡ç®—å“åº”æ—¶é—´
            duration_ms = (time.time() - start_time) * 1000
            response_size = len(response.content)
            
            # è®°å½•å“åº”
            logger.info(
                f"[REST_RSP] {response.status_code} {response.reason_phrase}, "
                f"size: {response_size} bytes, duration: {duration_ms:.2f}ms"
            )
            
            response.raise_for_status()
            return response.json()
            
    except httpx.HTTPStatusError as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.warning(
            f"[REST_ERR] HTTP {e.response.status_code}, "
            f"duration: {duration_ms:.2f}ms"
        )
        raise
    except httpx.TimeoutException as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.warning(f"[REST_ERR] Timeout after {duration_ms:.2f}ms")
        raise
    except httpx.ConnectError as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.warning(f"[REST_ERR] Connection failed: {e}, duration: {duration_ms:.2f}ms")
        raise
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.error(f"[REST_ERR] {type(e).__name__}: {e}, duration: {duration_ms:.2f}ms")
        raise


def _handle_error(e: Exception) -> str:
    """
    ç»Ÿä¸€é”™è¯¯å¤„ç†å‡½æ•°
    
    å°†å„ç§å¼‚å¸¸è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯ï¼Œ
    æä¾›æ˜ç¡®çš„é”™è¯¯åŸå› å’Œè§£å†³å»ºè®®ã€‚
    
    Args:
        e: æ•è·çš„å¼‚å¸¸
        
    Returns:
        æ ¼å¼åŒ–çš„é”™è¯¯æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    if isinstance(e, httpx.HTTPStatusError):
        status_code = e.response.status_code
        if status_code == 404:
            return "é”™è¯¯ï¼šèµ„æºæœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥ ID æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…è¯¥ä½œä¸š/ä»»åŠ¡å¯èƒ½å·²è¢«æ¸…ç†ã€‚"
        elif status_code == 403:
            return "é”™è¯¯ï¼šæƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®è¯¥èµ„æºã€‚è¯·æ£€æŸ¥è®¿é—®æƒé™é…ç½®ã€‚"
        elif status_code == 401:
            return "é”™è¯¯ï¼šè®¤è¯å¤±è´¥ã€‚å¦‚æœå¯ç”¨äº†å®‰å…¨æ¨¡å¼ï¼Œè¯·æ£€æŸ¥è®¤è¯é…ç½®ã€‚"
        elif status_code == 500:
            return "é”™è¯¯ï¼šæœåŠ¡å™¨å†…éƒ¨é”™è¯¯ã€‚è¯·æ£€æŸ¥ JobHistory Server æ—¥å¿—ã€‚"
        elif status_code == 503:
            return "é”™è¯¯ï¼šæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚JobHistory Server å¯èƒ½æ­£åœ¨å¯åŠ¨æˆ–è¿‡è½½ã€‚"
        return f"é”™è¯¯ï¼šAPI è¯·æ±‚å¤±è´¥ï¼ŒHTTP çŠ¶æ€ç  {status_code}ã€‚"
    elif isinstance(e, httpx.TimeoutException):
        return f"é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶ï¼ˆ{REQUEST_TIMEOUT}ç§’ï¼‰ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¢åŠ è¶…æ—¶æ—¶é—´ã€‚"
    elif isinstance(e, httpx.ConnectError):
        return f"é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° JobHistory Server ({JOBHISTORY_BASE_URL})ã€‚\nè¯·æ£€æŸ¥ï¼š\n1. æœåŠ¡æ˜¯å¦å·²å¯åŠ¨\n2. åœ°å€å’Œç«¯å£æ˜¯å¦æ­£ç¡®\n3. ç½‘ç»œæ˜¯å¦å¯è¾¾"
    return f"é”™è¯¯ï¼š{type(e).__name__} - {str(e)}"


def _format_timestamp(ms: int) -> str:
    """
    å°†æ¯«ç§’æ—¶é—´æˆ³è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼
    
    Args:
        ms: æ¯«ç§’æ—¶é—´æˆ³ï¼ˆè‡ª 1970-01-01 00:00:00 UTCï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¦‚ "2024-01-15 10:30:45"
        å¦‚æœæ—¶é—´æˆ³æ— æ•ˆï¼Œè¿”å› "N/A"
    """
    if not ms or ms <= 0:
        return "N/A"
    try:
        return datetime.fromtimestamp(ms / 1000).strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, OSError):
        return "N/A"


def _format_duration(ms: int) -> str:
    """
    å°†æ¯«ç§’æ—¶é•¿è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼
    
    Args:
        ms: æ¯«ç§’æ•°
        
    Returns:
        æ ¼å¼åŒ–çš„æ—¶é•¿å­—ç¬¦ä¸²ï¼Œå¦‚ "2æ—¶30åˆ†15ç§’"
    """
    if not ms or ms <= 0:
        return "N/A"
    
    seconds = ms // 1000
    if seconds < 60:
        return f"{seconds}ç§’"
    elif seconds < 3600:
        minutes = seconds // 60
        secs = seconds % 60
        return f"{minutes}åˆ†{secs}ç§’"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        secs = seconds % 60
        return f"{hours}æ—¶{minutes}åˆ†{secs}ç§’"


def _format_bytes(bytes_value: int) -> str:
    """
    å°†å­—èŠ‚æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼
    
    Args:
        bytes_value: å­—èŠ‚æ•°
        
    Returns:
        æ ¼å¼åŒ–çš„å¤§å°å­—ç¬¦ä¸²ï¼Œå¦‚ "1.5 GB"
    """
    if not bytes_value or bytes_value < 0:
        return "0 B"
    
    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
    unit_index = 0
    value = float(bytes_value)
    
    while value >= 1024 and unit_index < len(units) - 1:
        value /= 1024
        unit_index += 1
    
    if unit_index == 0:
        return f"{int(value)} {units[unit_index]}"
    return f"{value:.2f} {units[unit_index]}"


def _format_counters_markdown(counters_data: Dict[str, Any], title: str = "è®¡æ•°å™¨") -> str:
    """
    å°†è®¡æ•°å™¨æ•°æ®æ ¼å¼åŒ–ä¸º Markdown
    
    Args:
        counters_data: è®¡æ•°å™¨æ•°æ®å­—å…¸
        title: æ ‡é¢˜
        
    Returns:
        Markdown æ ¼å¼çš„è®¡æ•°å™¨ä¿¡æ¯
    """
    lines = [f"# {title}", ""]
    
    counter_groups = counters_data.get("counterGroup", [])
    if not counter_groups:
        counter_groups = counters_data.get("taskCounterGroup", [])
    if not counter_groups:
        counter_groups = counters_data.get("taskAttemptCounterGroup", [])
    
    for group in counter_groups:
        group_name = group.get("counterGroupName", "Unknown Group")
        # ç®€åŒ–ç»„åæ˜¾ç¤º
        short_name = group_name.split(".")[-1] if "." in group_name else group_name
        lines.append(f"## {short_name}")
        lines.append("")
        
        counters = group.get("counter", [])
        for counter in counters:
            name = counter.get("name", "Unknown")
            # å°è¯•è·å–ä¸åŒç±»å‹çš„å€¼
            total_value = counter.get("totalCounterValue", counter.get("value", 0))
            map_value = counter.get("mapCounterValue")
            reduce_value = counter.get("reduceCounterValue")
            
            if map_value is not None and reduce_value is not None:
                lines.append(f"- **{name}**: {total_value:,} (Map: {map_value:,}, Reduce: {reduce_value:,})")
            else:
                lines.append(f"- **{name}**: {total_value:,}")
        lines.append("")
    
    return "\n".join(lines)


def _extract_hostname(node_http_address: str) -> str:
    """
    ä» nodeHttpAddress æå–ä¸»æœºå
    
    Args:
        node_http_address: èŠ‚ç‚¹ HTTP åœ°å€ï¼Œæ ¼å¼å¦‚ "hostname:port"
        
    Returns:
        ä¸»æœºåéƒ¨åˆ†
        
    Example:
        è¾“å…¥: pro-hadooptemporary-dc01-085025.vm.dc01.hellocloud.tech:8042
        è¾“å‡º: pro-hadooptemporary-dc01-085025.vm.dc01.hellocloud.tech
    """
    if ':' in node_http_address:
        return node_http_address.rsplit(':', 1)[0]
    return node_http_address


async def _fetch_logs_html(url: str) -> str:
    """
    è·å–æ—¥å¿— HTML å†…å®¹
    
    å‘é€ HTTP GET è¯·æ±‚è·å–æ—¥å¿—é¡µé¢çš„ HTML å†…å®¹ã€‚
    
    Args:
        url: æ—¥å¿— URL
        
    Returns:
        HTML å†…å®¹å­—ç¬¦ä¸²
        
    Raises:
        httpx.HTTPStatusError: HTTP é”™è¯¯çŠ¶æ€ç 
        httpx.TimeoutException: è¯·æ±‚è¶…æ—¶
        httpx.ConnectError: è¿æ¥å¤±è´¥
    """
    logger.info(f"[REST_REQ] GET {url}")
    start_time = time.time()
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                url,
                timeout=REQUEST_TIMEOUT,
                headers={"Accept": "text/html"},
                follow_redirects=True
            )
            
            duration_ms = (time.time() - start_time) * 1000
            logger.info(
                f"[REST_RSP] {response.status_code} {response.reason_phrase}, "
                f"size: {len(response.content)} bytes, duration: {duration_ms:.2f}ms"
            )
            
            response.raise_for_status()
            return response.text
            
    except httpx.HTTPStatusError as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.warning(
            f"[REST_ERR] HTTP {e.response.status_code}, "
            f"duration: {duration_ms:.2f}ms"
        )
        raise
    except httpx.TimeoutException:
        duration_ms = (time.time() - start_time) * 1000
        logger.warning(f"[REST_ERR] Timeout after {duration_ms:.2f}ms")
        raise
    except httpx.ConnectError as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.warning(f"[REST_ERR] Connection failed: {e}, duration: {duration_ms:.2f}ms")
        raise
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.error(f"[REST_ERR] {type(e).__name__}: {e}, duration: {duration_ms:.2f}ms")
        raise


def _extract_pre_content(html: str) -> str:
    """
    ä» HTML ä¸­æå– <pre> æ ‡ç­¾çš„å†…å®¹
    
    Args:
        html: HTML å†…å®¹å­—ç¬¦ä¸²
        
    Returns:
        <pre> æ ‡ç­¾ä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    # ä½¿ç”¨æ­£åˆ™åŒ¹é… <pre>...</pre> å†…å®¹
    match = re.search(r'<pre[^>]*>(.*?)</pre>', html, re.DOTALL | re.IGNORECASE)
    if match:
        content = match.group(1)
        # å¤„ç† HTML å®ä½“
        content = content.replace('&lt;', '<')
        content = content.replace('&gt;', '>')
        content = content.replace('&amp;', '&')
        content = content.replace('&quot;', '"')
        content = content.replace('&#39;', "'")
        content = content.replace('&nbsp;', ' ')
        return content.strip()
    return ""


# ==============================================================================
# MCP å·¥å…·å®šä¹‰
# ==============================================================================


@mcp.tool(
    name="jobhistory_get_info",
    annotations={
        "title": "è·å– History Server ä¿¡æ¯",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_info() -> str:
    """
    è·å– Hadoop JobHistory Server çš„åŸºæœ¬ä¿¡æ¯ã€‚
    
    è¿”å›æœåŠ¡å™¨å¯åŠ¨æ—¶é—´ã€Hadoop ç‰ˆæœ¬ã€æ„å»ºä¿¡æ¯ç­‰ã€‚
    è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¥åº·æ£€æŸ¥å·¥å…·ï¼Œå¯ä»¥éªŒè¯æœåŠ¡æ˜¯å¦å¯ç”¨ã€‚
    
    Returns:
        str: Markdown æ ¼å¼çš„æœåŠ¡å™¨ä¿¡æ¯
        
    Example:
        ä½¿ç”¨æ­¤å·¥å…·æ£€æŸ¥ JobHistory Server æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚
    """
    try:
        data = await _make_request("info")
        info = data.get("historyInfo", {})
        
        result = f"""# JobHistory Server ä¿¡æ¯

## æœåŠ¡çŠ¶æ€
- **å¯åŠ¨æ—¶é—´**: {_format_timestamp(info.get('startedOn', 0))}
- **è¿è¡ŒçŠ¶æ€**: æ­£å¸¸

## Hadoop ç‰ˆæœ¬ä¿¡æ¯
- **ç‰ˆæœ¬**: {info.get('hadoopVersion', 'N/A')}
- **æ„å»ºç‰ˆæœ¬**: {info.get('hadoopBuildVersion', 'N/A')}
- **æ„å»ºæ—¶é—´**: {info.get('hadoopVersionBuiltOn', 'N/A')}

## è¿æ¥ä¿¡æ¯
- **æœåŠ¡åœ°å€**: {JOBHISTORY_BASE_URL}
"""
        return result
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_list_jobs",
    annotations={
        "title": "åˆ—å‡º MapReduce ä½œä¸š",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_list_jobs(params: ListJobsInput) -> str:
    """
    åˆ—å‡ºå·²å®Œæˆçš„ MapReduce ä½œä¸šã€‚
    
    æ”¯æŒå¤šç§è¿‡æ»¤æ¡ä»¶ï¼š
    - æŒ‰ç”¨æˆ·åè¿‡æ»¤
    - æŒ‰ä½œä¸šçŠ¶æ€è¿‡æ»¤
    - æŒ‰é˜Ÿåˆ—åè¿‡æ»¤
    - æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤
    
    æ”¯æŒåˆ†é¡µï¼Œé»˜è®¤è¿”å› 20 æ¡è®°å½•ã€‚
    
    Args:
        params (ListJobsInput): æŸ¥è¯¢å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
            - user: ç”¨æˆ·åè¿‡æ»¤
            - state: çŠ¶æ€è¿‡æ»¤ï¼ˆSUCCEEDED, FAILED, KILLED ç­‰ï¼‰
            - queue: é˜Ÿåˆ—åè¿‡æ»¤
            - limit: è¿”å›æ•°é‡é™åˆ¶
            - started_time_begin/end: å¼€å§‹æ—¶é—´èŒƒå›´
            - finished_time_begin/end: ç»“æŸæ—¶é—´èŒƒå›´
            - response_format: è¾“å‡ºæ ¼å¼
    
    Returns:
        str: ä½œä¸šåˆ—è¡¨ï¼ŒMarkdown æˆ– JSON æ ¼å¼
        
    Examples:
        - æŸ¥è¯¢æ‰€æœ‰ä½œä¸š: ä½¿ç”¨é»˜è®¤å‚æ•°
        - æŸ¥è¯¢å¤±è´¥çš„ä½œä¸š: state="FAILED"
        - æŸ¥è¯¢ç‰¹å®šç”¨æˆ·çš„ä½œä¸š: user="hadoop"
    """
    try:
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        query_params = {}
        if params.user:
            query_params["user"] = params.user
        if params.state:
            query_params["state"] = params.state.value
        if params.queue:
            query_params["queue"] = params.queue
        if params.limit:
            query_params["limit"] = params.limit
        if params.started_time_begin:
            query_params["startedTimeBegin"] = params.started_time_begin
        if params.started_time_end:
            query_params["startedTimeEnd"] = params.started_time_end
        if params.finished_time_begin:
            query_params["finishedTimeBegin"] = params.finished_time_begin
        if params.finished_time_end:
            query_params["finishedTimeEnd"] = params.finished_time_end

        data = await _make_request("mapreduce/jobs", query_params)
        jobs = data.get("jobs", {}).get("job", [])

        if not jobs:
            return "æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä½œä¸šã€‚"

        # JSON æ ¼å¼è¾“å‡º
        if params.response_format == ResponseFormat.JSON:
            return json.dumps({
                "total": len(jobs),
                "jobs": jobs
            }, indent=2, ensure_ascii=False)

        # Markdown æ ¼å¼è¾“å‡º
        lines = [
            "# MapReduce ä½œä¸šåˆ—è¡¨",
            f"å…±æ‰¾åˆ° **{len(jobs)}** ä¸ªä½œä¸š",
            ""
        ]

        for job in jobs:
            job_id = job.get('id', 'N/A')
            job_name = job.get('name', 'N/A')
            state = job.get('state', 'N/A')
            user = job.get('user', 'N/A')
            queue = job.get('queue', 'N/A')
            
            # çŠ¶æ€å›¾æ ‡
            state_icon = {
                'SUCCEEDED': 'âœ…',
                'FAILED': 'âŒ',
                'KILLED': 'âš ï¸',
                'RUNNING': 'ğŸ”„'
            }.get(state, 'â“')
            
            lines.append(f"## {state_icon} {job_name}")
            lines.append(f"**ID**: `{job_id}`")
            lines.append("")
            lines.append(f"| å±æ€§ | å€¼ |")
            lines.append(f"|------|-----|")
            lines.append(f"| ç”¨æˆ· | {user} |")
            lines.append(f"| é˜Ÿåˆ— | {queue} |")
            lines.append(f"| çŠ¶æ€ | {state} |")
            lines.append(f"| å¼€å§‹æ—¶é—´ | {_format_timestamp(job.get('startTime', 0))} |")
            lines.append(f"| ç»“æŸæ—¶é—´ | {_format_timestamp(job.get('finishTime', 0))} |")
            lines.append(f"| Map è¿›åº¦ | {job.get('mapsCompleted', 0)}/{job.get('mapsTotal', 0)} |")
            lines.append(f"| Reduce è¿›åº¦ | {job.get('reducesCompleted', 0)}/{job.get('reducesTotal', 0)} |")
            lines.append("")

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_job",
    annotations={
        "title": "è·å–ä½œä¸šè¯¦æƒ…",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_job(params: GetJobInput) -> str:
    """
    è·å–æŒ‡å®š MapReduce ä½œä¸šçš„è¯¦ç»†ä¿¡æ¯ã€‚
    
    åŒ…æ‹¬ä½œä¸šçš„å®Œæ•´å…ƒæ•°æ®ï¼š
    - åŸºæœ¬ä¿¡æ¯ï¼ˆIDã€åç§°ã€ç”¨æˆ·ã€é˜Ÿåˆ—ã€çŠ¶æ€ï¼‰
    - æ—¶é—´ä¿¡æ¯ï¼ˆæäº¤ã€å¼€å§‹ã€ç»“æŸæ—¶é—´ï¼‰
    - ä»»åŠ¡ç»Ÿè®¡ï¼ˆMap/Reduce æ•°é‡ã€æˆåŠŸ/å¤±è´¥æ•°ï¼‰
    - æ€§èƒ½ç»Ÿè®¡ï¼ˆå¹³å‡æ‰§è¡Œæ—¶é—´ï¼‰
    - è®¿é—®æ§åˆ¶åˆ—è¡¨ï¼ˆACLï¼‰
    
    Args:
        params (GetJobInput): åŒ…å« job_id çš„è¾“å…¥å‚æ•°
    
    Returns:
        str: ä½œä¸šè¯¦æƒ…ï¼ŒMarkdown æˆ– JSON æ ¼å¼
        
    Examples:
        - è·å–ä½œä¸šè¯¦æƒ…: job_id="job_1326381300833_2_2"
    """
    try:
        data = await _make_request(f"mapreduce/jobs/{params.job_id}")
        job = data.get("job", {})

        if params.response_format == ResponseFormat.JSON:
            return json.dumps(job, indent=2, ensure_ascii=False)

        state = job.get('state', 'N/A')
        state_icon = {
            'SUCCEEDED': 'âœ…',
            'FAILED': 'âŒ',
            'KILLED': 'âš ï¸',
            'RUNNING': 'ğŸ”„'
        }.get(state, 'â“')

        lines = [
            f"# {state_icon} ä½œä¸šè¯¦æƒ…: {job.get('name', 'N/A')}",
            "",
            "## åŸºæœ¬ä¿¡æ¯",
            f"| å±æ€§ | å€¼ |",
            f"|------|-----|",
            f"| ä½œä¸š ID | `{job.get('id', 'N/A')}` |",
            f"| ä½œä¸šåç§° | {job.get('name', 'N/A')} |",
            f"| ç”¨æˆ· | {job.get('user', 'N/A')} |",
            f"| é˜Ÿåˆ— | {job.get('queue', 'N/A')} |",
            f"| çŠ¶æ€ | {state} |",
            f"| Uber æ¨¡å¼ | {'æ˜¯' if job.get('uberized') else 'å¦'} |",
            "",
            "## æ—¶é—´ä¿¡æ¯",
            f"| é˜¶æ®µ | æ—¶é—´ |",
            f"|------|-----|",
            f"| æäº¤æ—¶é—´ | {_format_timestamp(job.get('submitTime', 0))} |",
            f"| å¼€å§‹æ—¶é—´ | {_format_timestamp(job.get('startTime', 0))} |",
            f"| ç»“æŸæ—¶é—´ | {_format_timestamp(job.get('finishTime', 0))} |",
            "",
            "## ä»»åŠ¡ç»Ÿè®¡",
            f"| ç±»å‹ | å®Œæˆ/æ€»æ•° | æˆåŠŸ | å¤±è´¥ | ç»ˆæ­¢ |",
            f"|------|----------|------|------|------|",
            f"| Map | {job.get('mapsCompleted', 0)}/{job.get('mapsTotal', 0)} | {job.get('successfulMapAttempts', 0)} | {job.get('failedMapAttempts', 0)} | {job.get('killedMapAttempts', 0)} |",
            f"| Reduce | {job.get('reducesCompleted', 0)}/{job.get('reducesTotal', 0)} | {job.get('successfulReduceAttempts', 0)} | {job.get('failedReduceAttempts', 0)} | {job.get('killedReduceAttempts', 0)} |",
            "",
            "## æ€§èƒ½ç»Ÿè®¡",
            f"| æŒ‡æ ‡ | è€—æ—¶ |",
            f"|------|------|",
            f"| å¹³å‡ Map æ—¶é—´ | {_format_duration(job.get('avgMapTime', 0))} |",
            f"| å¹³å‡ Reduce æ—¶é—´ | {_format_duration(job.get('avgReduceTime', 0))} |",
            f"| å¹³å‡ Shuffle æ—¶é—´ | {_format_duration(job.get('avgShuffleTime', 0))} |",
            f"| å¹³å‡ Merge æ—¶é—´ | {_format_duration(job.get('avgMergeTime', 0))} |",
        ]

        # è¯Šæ–­ä¿¡æ¯
        diagnostics = job.get('diagnostics')
        if diagnostics:
            lines.extend([
                "",
                "## è¯Šæ–­ä¿¡æ¯",
                f"```",
                diagnostics,
                f"```"
            ])

        # ACL ä¿¡æ¯
        acls = job.get('acls', [])
        if acls:
            lines.extend([
                "",
                "## è®¿é—®æ§åˆ¶",
                f"| ACL åç§° | å€¼ |",
                f"|----------|-----|"
            ])
            for acl in acls:
                lines.append(f"| {acl.get('name', 'N/A')} | {acl.get('value', 'N/A')} |")

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_job_counters",
    annotations={
        "title": "è·å–ä½œä¸šè®¡æ•°å™¨",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_job_counters(params: GetJobCountersInput) -> str:
    """
    è·å–æŒ‡å®šä½œä¸šçš„æ‰€æœ‰è®¡æ•°å™¨ä¿¡æ¯ã€‚
    
    è®¡æ•°å™¨åŒ…å«ä½œä¸šæ‰§è¡Œçš„è¯¦ç»†ç»Ÿè®¡æ•°æ®ï¼š
    - æ–‡ä»¶ç³»ç»Ÿè®¡æ•°å™¨ï¼ˆè¯»å†™å­—èŠ‚æ•°ã€æ“ä½œæ•°ï¼‰
    - ä»»åŠ¡è®¡æ•°å™¨ï¼ˆè¾“å…¥è¾“å‡ºè®°å½•æ•°ã€æº¢å‡ºè®°å½•æ•°ï¼‰
    - Shuffle é”™è¯¯è®¡æ•°å™¨
    - è‡ªå®šä¹‰è®¡æ•°å™¨
    
    Args:
        params (GetJobCountersInput): åŒ…å« job_id çš„è¾“å…¥å‚æ•°
    
    Returns:
        str: è®¡æ•°å™¨ä¿¡æ¯ï¼ŒæŒ‰ç»„åˆ†ç±»å±•ç¤º
    """
    try:
        data = await _make_request(f"mapreduce/jobs/{params.job_id}/counters")
        counters = data.get("jobCounters", {})

        if params.response_format == ResponseFormat.JSON:
            return json.dumps(counters, indent=2, ensure_ascii=False)

        job_id = counters.get('id', params.job_id)
        return _format_counters_markdown(
            counters,
            f"ä½œä¸šè®¡æ•°å™¨: {job_id}"
        )
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_job_conf",
    annotations={
        "title": "è·å–ä½œä¸šé…ç½®",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_job_conf(params: GetJobConfInput) -> str:
    """
    è·å–æŒ‡å®šä½œä¸šçš„é…ç½®ä¿¡æ¯ã€‚
    
    è¿”å›ä½œä¸šè¿è¡Œæ—¶ä½¿ç”¨çš„æ‰€æœ‰é…ç½®å‚æ•°ï¼Œ
    åŒ…æ‹¬ Hadoop é»˜è®¤é…ç½®ã€ç«™ç‚¹é…ç½®å’Œä½œä¸šç‰¹å®šé…ç½®ã€‚
    
    å¯ä»¥é€šè¿‡ filter_key å‚æ•°è¿‡æ»¤é…ç½®é¡¹ã€‚
    
    Args:
        params (GetJobConfInput): åŒ…å« job_id å’Œå¯é€‰çš„ filter_key
    
    Returns:
        str: é…ç½®ä¿¡æ¯åˆ—è¡¨
        
    Examples:
        - è·å–æ‰€æœ‰é…ç½®: job_id="job_xxx"
        - è¿‡æ»¤ MapReduce é…ç½®: job_id="job_xxx", filter_key="mapreduce"
    """
    try:
        data = await _make_request(f"mapreduce/jobs/{params.job_id}/conf")
        conf = data.get("conf", {})

        if params.response_format == ResponseFormat.JSON:
            return json.dumps(conf, indent=2, ensure_ascii=False)

        path = conf.get('path', 'N/A')
        properties = conf.get('property', [])

        # åº”ç”¨è¿‡æ»¤
        if params.filter_key:
            filter_lower = params.filter_key.lower()
            properties = [
                p for p in properties
                if filter_lower in p.get('name', '').lower()
            ]

        lines = [
            f"# ä½œä¸šé…ç½®: {params.job_id}",
            f"**é…ç½®æ–‡ä»¶è·¯å¾„**: `{path}`",
            "",
            f"å…± **{len(properties)}** ä¸ªé…ç½®é¡¹" + (f"ï¼ˆè¿‡æ»¤: '{params.filter_key}'ï¼‰" if params.filter_key else ""),
            ""
        ]

        # æŒ‰é…ç½®åç§°å‰ç¼€åˆ†ç»„
        groups: Dict[str, List[Dict]] = {}
        for prop in properties:
            name = prop.get('name', '')
            prefix = name.split('.')[0] if '.' in name else 'other'
            if prefix not in groups:
                groups[prefix] = []
            groups[prefix].append(prop)

        for prefix in sorted(groups.keys()):
            props = groups[prefix]
            lines.append(f"## {prefix} ({len(props)} é¡¹)")
            lines.append("")
            for prop in props:
                name = prop.get('name', 'N/A')
                value = prop.get('value', 'N/A')
                # æˆªæ–­è¿‡é•¿çš„å€¼
                if len(value) > 100:
                    value = value[:100] + "..."
                lines.append(f"- `{name}` = `{value}`")
            lines.append("")

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_job_attempts",
    annotations={
        "title": "è·å–ä½œä¸šå°è¯•åˆ—è¡¨",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_job_attempts(params: GetJobAttemptsInput) -> str:
    """
    è·å–æŒ‡å®šä½œä¸šçš„ ApplicationMaster å°è¯•åˆ—è¡¨ã€‚
    
    å½“ä½œä¸šçš„ AM å¤±è´¥æ—¶ï¼ŒYARN ä¼šé‡æ–°å¯åŠ¨ AMï¼Œ
    æ¯æ¬¡å¯åŠ¨éƒ½æ˜¯ä¸€ä¸ªæ–°çš„å°è¯•ã€‚æ­¤å·¥å…·è¿”å›æ‰€æœ‰å°è¯•çš„ä¿¡æ¯ã€‚
    
    Args:
        params (GetJobAttemptsInput): åŒ…å« job_id
    
    Returns:
        str: AM å°è¯•åˆ—è¡¨
    """
    try:
        data = await _make_request(f"mapreduce/jobs/{params.job_id}/jobattempts")
        attempts = data.get("jobAttempts", {}).get("jobAttempt", [])

        if params.response_format == ResponseFormat.JSON:
            return json.dumps({
                "total": len(attempts),
                "attempts": attempts
            }, indent=2, ensure_ascii=False)

        if not attempts:
            return "æ²¡æœ‰æ‰¾åˆ°ä½œä¸šå°è¯•è®°å½•ã€‚"

        lines = [
            f"# ä½œä¸šå°è¯•åˆ—è¡¨: {params.job_id}",
            f"å…± **{len(attempts)}** æ¬¡å°è¯•",
            ""
        ]

        for attempt in attempts:
            attempt_id = attempt.get('id', 'N/A')
            lines.append(f"## å°è¯• #{attempt_id}")
            lines.append("")
            lines.append(f"| å±æ€§ | å€¼ |")
            lines.append(f"|------|-----|")
            lines.append(f"| å®¹å™¨ ID | `{attempt.get('containerId', 'N/A')}` |")
            lines.append(f"| èŠ‚ç‚¹ ID | {attempt.get('nodeId', 'N/A')} |")
            lines.append(f"| èŠ‚ç‚¹ HTTP åœ°å€ | {attempt.get('nodeHttpAddress', 'N/A')} |")
            lines.append(f"| å¼€å§‹æ—¶é—´ | {_format_timestamp(attempt.get('startTime', 0))} |")
            
            logs_link = attempt.get('logsLink', '')
            if logs_link:
                lines.append(f"| æ—¥å¿—é“¾æ¥ | [æŸ¥çœ‹æ—¥å¿—]({logs_link}) |")
            lines.append("")

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_list_tasks",
    annotations={
        "title": "åˆ—å‡ºä½œä¸šä»»åŠ¡",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_list_tasks(params: ListTasksInput) -> str:
    """
    åˆ—å‡ºæŒ‡å®šä½œä¸šçš„æ‰€æœ‰ä»»åŠ¡ã€‚
    
    å¯ä»¥æŒ‰ä»»åŠ¡ç±»å‹ï¼ˆMap æˆ– Reduceï¼‰è¿‡æ»¤ã€‚
    
    Args:
        params (ListTasksInput): åŒ…å« job_id å’Œå¯é€‰çš„ task_type
    
    Returns:
        str: ä»»åŠ¡åˆ—è¡¨
        
    Examples:
        - åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡: job_id="job_xxx"
        - åªåˆ—å‡º Map ä»»åŠ¡: job_id="job_xxx", task_type="m"
        - åªåˆ—å‡º Reduce ä»»åŠ¡: job_id="job_xxx", task_type="r"
    """
    try:
        query_params = {}
        if params.task_type:
            query_params["type"] = params.task_type.value

        data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks",
            query_params
        )
        tasks = data.get("tasks", {}).get("task", [])

        if params.response_format == ResponseFormat.JSON:
            return json.dumps({
                "total": len(tasks),
                "tasks": tasks
            }, indent=2, ensure_ascii=False)

        if not tasks:
            return "æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡ã€‚"

        lines = [
            f"# ä»»åŠ¡åˆ—è¡¨: {params.job_id}",
            f"å…± **{len(tasks)}** ä¸ªä»»åŠ¡",
            ""
        ]

        # æŒ‰ç±»å‹åˆ†ç»„
        map_tasks = [t for t in tasks if t.get('type') == 'MAP']
        reduce_tasks = [t for t in tasks if t.get('type') == 'REDUCE']

        if map_tasks:
            lines.append(f"## Map ä»»åŠ¡ ({len(map_tasks)} ä¸ª)")
            lines.append("")
            lines.append("| ä»»åŠ¡ ID | çŠ¶æ€ | è¿›åº¦ | è€—æ—¶ |")
            lines.append("|---------|------|------|------|")
            for task in map_tasks:
                task_id = task.get('id', 'N/A')
                state = task.get('state', 'N/A')
                progress = task.get('progress', 0)
                elapsed = _format_duration(task.get('elapsedTime', 0))
                lines.append(f"| `{task_id}` | {state} | {progress:.1f}% | {elapsed} |")
            lines.append("")

        if reduce_tasks:
            lines.append(f"## Reduce ä»»åŠ¡ ({len(reduce_tasks)} ä¸ª)")
            lines.append("")
            lines.append("| ä»»åŠ¡ ID | çŠ¶æ€ | è¿›åº¦ | è€—æ—¶ |")
            lines.append("|---------|------|------|------|")
            for task in reduce_tasks:
                task_id = task.get('id', 'N/A')
                state = task.get('state', 'N/A')
                progress = task.get('progress', 0)
                elapsed = _format_duration(task.get('elapsedTime', 0))
                lines.append(f"| `{task_id}` | {state} | {progress:.1f}% | {elapsed} |")
            lines.append("")

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_task",
    annotations={
        "title": "è·å–ä»»åŠ¡è¯¦æƒ…",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_task(params: GetTaskInput) -> str:
    """
    è·å–æŒ‡å®šä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚
    
    Args:
        params (GetTaskInput): åŒ…å« job_id å’Œ task_id
    
    Returns:
        str: ä»»åŠ¡è¯¦æƒ…
    """
    try:
        data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks/{params.task_id}"
        )
        task = data.get("task", {})

        if params.response_format == ResponseFormat.JSON:
            return json.dumps(task, indent=2, ensure_ascii=False)

        task_type = task.get('type', 'UNKNOWN')
        state = task.get('state', 'N/A')
        state_icon = {
            'SUCCEEDED': 'âœ…',
            'FAILED': 'âŒ',
            'KILLED': 'âš ï¸',
            'RUNNING': 'ğŸ”„'
        }.get(state, 'â“')

        lines = [
            f"# {state_icon} ä»»åŠ¡è¯¦æƒ…: {task.get('id', 'N/A')}",
            "",
            f"| å±æ€§ | å€¼ |",
            f"|------|-----|",
            f"| ä»»åŠ¡ ID | `{task.get('id', 'N/A')}` |",
            f"| ç±»å‹ | {task_type} |",
            f"| çŠ¶æ€ | {state} |",
            f"| è¿›åº¦ | {task.get('progress', 0):.1f}% |",
            f"| å¼€å§‹æ—¶é—´ | {_format_timestamp(task.get('startTime', 0))} |",
            f"| ç»“æŸæ—¶é—´ | {_format_timestamp(task.get('finishTime', 0))} |",
            f"| è€—æ—¶ | {_format_duration(task.get('elapsedTime', 0))} |",
            f"| æˆåŠŸå°è¯• | `{task.get('successfulAttempt', 'N/A')}` |",
        ]

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_task_counters",
    annotations={
        "title": "è·å–ä»»åŠ¡è®¡æ•°å™¨",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_task_counters(params: GetTaskCountersInput) -> str:
    """
    è·å–æŒ‡å®šä»»åŠ¡çš„è®¡æ•°å™¨ä¿¡æ¯ã€‚
    
    Args:
        params (GetTaskCountersInput): åŒ…å« job_id å’Œ task_id
    
    Returns:
        str: ä»»åŠ¡è®¡æ•°å™¨ä¿¡æ¯
    """
    try:
        data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks/{params.task_id}/counters"
        )
        counters = data.get("jobTaskCounters", {})

        if params.response_format == ResponseFormat.JSON:
            return json.dumps(counters, indent=2, ensure_ascii=False)

        task_id = counters.get('id', params.task_id)
        return _format_counters_markdown(
            counters,
            f"ä»»åŠ¡è®¡æ•°å™¨: {task_id}"
        )
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_list_task_attempts",
    annotations={
        "title": "åˆ—å‡ºä»»åŠ¡å°è¯•",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_list_task_attempts(params: ListTaskAttemptsInput) -> str:
    """
    åˆ—å‡ºæŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰å°è¯•ã€‚
    
    å½“ä»»åŠ¡å¤±è´¥æ—¶ä¼šè¿›è¡Œé‡è¯•ï¼Œæ¯æ¬¡é‡è¯•éƒ½æ˜¯ä¸€ä¸ªæ–°çš„å°è¯•ã€‚
    
    Args:
        params (ListTaskAttemptsInput): åŒ…å« job_id å’Œ task_id
    
    Returns:
        str: ä»»åŠ¡å°è¯•åˆ—è¡¨
    """
    try:
        data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks/{params.task_id}/attempts"
        )
        attempts = data.get("taskAttempts", {}).get("taskAttempt", [])

        if params.response_format == ResponseFormat.JSON:
            return json.dumps({
                "total": len(attempts),
                "attempts": attempts
            }, indent=2, ensure_ascii=False)

        if not attempts:
            return "æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡å°è¯•è®°å½•ã€‚"

        lines = [
            f"# ä»»åŠ¡å°è¯•åˆ—è¡¨",
            f"**ä»»åŠ¡ ID**: `{params.task_id}`",
            f"å…± **{len(attempts)}** æ¬¡å°è¯•",
            ""
        ]

        for attempt in attempts:
            attempt_id = attempt.get('id', 'N/A')
            state = attempt.get('state', 'N/A')
            state_icon = {
                'SUCCEEDED': 'âœ…',
                'FAILED': 'âŒ',
                'KILLED': 'âš ï¸'
            }.get(state, 'â“')

            lines.append(f"## {state_icon} {attempt_id}")
            lines.append("")
            lines.append(f"| å±æ€§ | å€¼ |")
            lines.append(f"|------|-----|")
            lines.append(f"| çŠ¶æ€ | {state} |")
            lines.append(f"| ç±»å‹ | {attempt.get('type', 'N/A')} |")
            lines.append(f"| è¿›åº¦ | {attempt.get('progress', 0):.1f}% |")
            lines.append(f"| å®¹å™¨ ID | `{attempt.get('assignedContainerId', 'N/A')}` |")
            lines.append(f"| èŠ‚ç‚¹ | {attempt.get('nodeHttpAddress', 'N/A')} |")
            lines.append(f"| æœºæ¶ | {attempt.get('rack', 'N/A')} |")
            lines.append(f"| å¼€å§‹æ—¶é—´ | {_format_timestamp(attempt.get('startTime', 0))} |")
            lines.append(f"| ç»“æŸæ—¶é—´ | {_format_timestamp(attempt.get('finishTime', 0))} |")
            lines.append(f"| è€—æ—¶ | {_format_duration(attempt.get('elapsedTime', 0))} |")
            
            diagnostics = attempt.get('diagnostics')
            if diagnostics:
                lines.append(f"| è¯Šæ–­ä¿¡æ¯ | {diagnostics} |")
            lines.append("")

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_task_attempt",
    annotations={
        "title": "è·å–ä»»åŠ¡å°è¯•è¯¦æƒ…",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_task_attempt(params: GetTaskAttemptInput) -> str:
    """
    è·å–æŒ‡å®šä»»åŠ¡å°è¯•çš„è¯¦ç»†ä¿¡æ¯ã€‚
    
    å¯¹äº Reduce ä»»åŠ¡å°è¯•ï¼Œè¿˜åŒ…å« Shuffle å’Œ Merge é˜¶æ®µçš„æ—¶é—´ä¿¡æ¯ã€‚
    
    Args:
        params (GetTaskAttemptInput): åŒ…å« job_id, task_id å’Œ attempt_id
    
    Returns:
        str: ä»»åŠ¡å°è¯•è¯¦æƒ…
    """
    try:
        data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks/{params.task_id}/attempts/{params.attempt_id}"
        )
        attempt = data.get("taskAttempt", {})

        if params.response_format == ResponseFormat.JSON:
            return json.dumps(attempt, indent=2, ensure_ascii=False)

        state = attempt.get('state', 'N/A')
        state_icon = {
            'SUCCEEDED': 'âœ…',
            'FAILED': 'âŒ',
            'KILLED': 'âš ï¸'
        }.get(state, 'â“')

        lines = [
            f"# {state_icon} ä»»åŠ¡å°è¯•è¯¦æƒ…",
            f"**å°è¯• ID**: `{attempt.get('id', 'N/A')}`",
            "",
            "## åŸºæœ¬ä¿¡æ¯",
            f"| å±æ€§ | å€¼ |",
            f"|------|-----|",
            f"| çŠ¶æ€ | {state} |",
            f"| ç±»å‹ | {attempt.get('type', 'N/A')} |",
            f"| è¿›åº¦ | {attempt.get('progress', 0):.1f}% |",
            "",
            "## æ‰§è¡Œç¯å¢ƒ",
            f"| å±æ€§ | å€¼ |",
            f"|------|-----|",
            f"| å®¹å™¨ ID | `{attempt.get('assignedContainerId', 'N/A')}` |",
            f"| èŠ‚ç‚¹åœ°å€ | {attempt.get('nodeHttpAddress', 'N/A')} |",
            f"| æœºæ¶ | {attempt.get('rack', 'N/A')} |",
            "",
            "## æ—¶é—´ä¿¡æ¯",
            f"| é˜¶æ®µ | æ—¶é—´/è€—æ—¶ |",
            f"|------|----------|",
            f"| å¼€å§‹æ—¶é—´ | {_format_timestamp(attempt.get('startTime', 0))} |",
            f"| ç»“æŸæ—¶é—´ | {_format_timestamp(attempt.get('finishTime', 0))} |",
            f"| æ€»è€—æ—¶ | {_format_duration(attempt.get('elapsedTime', 0))} |",
        ]

        # Reduce ä»»åŠ¡ç‰¹æœ‰çš„é˜¶æ®µæ—¶é—´
        if attempt.get('type') == 'REDUCE':
            lines.append(f"| Shuffle å®Œæˆæ—¶é—´ | {_format_timestamp(attempt.get('shuffleFinishTime', 0))} |")
            lines.append(f"| Merge å®Œæˆæ—¶é—´ | {_format_timestamp(attempt.get('mergeFinishTime', 0))} |")
            lines.append(f"| Shuffle è€—æ—¶ | {_format_duration(attempt.get('elapsedShuffleTime', 0))} |")
            lines.append(f"| Merge è€—æ—¶ | {_format_duration(attempt.get('elapsedMergeTime', 0))} |")
            lines.append(f"| Reduce è€—æ—¶ | {_format_duration(attempt.get('elapsedReduceTime', 0))} |")

        diagnostics = attempt.get('diagnostics')
        if diagnostics:
            lines.extend([
                "",
                "## è¯Šæ–­ä¿¡æ¯",
                "```",
                diagnostics,
                "```"
            ])

        return "\n".join(lines)
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_task_attempt_counters",
    annotations={
        "title": "è·å–ä»»åŠ¡å°è¯•è®¡æ•°å™¨",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_task_attempt_counters(params: GetTaskAttemptCountersInput) -> str:
    """
    è·å–æŒ‡å®šä»»åŠ¡å°è¯•çš„è®¡æ•°å™¨ä¿¡æ¯ã€‚
    
    Args:
        params (GetTaskAttemptCountersInput): åŒ…å« job_id, task_id å’Œ attempt_id
    
    Returns:
        str: ä»»åŠ¡å°è¯•è®¡æ•°å™¨ä¿¡æ¯
    """
    try:
        data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks/{params.task_id}/attempts/{params.attempt_id}/counters"
        )
        counters = data.get("jobTaskAttemptCounters", {})

        if params.response_format == ResponseFormat.JSON:
            return json.dumps(counters, indent=2, ensure_ascii=False)

        attempt_id = counters.get('id', params.attempt_id)
        return _format_counters_markdown(
            counters,
            f"ä»»åŠ¡å°è¯•è®¡æ•°å™¨: {attempt_id}"
        )
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_task_attempt_logs",
    annotations={
        "title": "è·å–ä»»åŠ¡å°è¯•æ—¥å¿—",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_task_attempt_logs(params: GetTaskAttemptLogsInput) -> str:
    """
    è·å–æŒ‡å®šä»»åŠ¡å°è¯•çš„å®¹å™¨æ—¥å¿—å†…å®¹ã€‚
    
    è¯¥å·¥å…·é€šè¿‡ä»¥ä¸‹æ­¥éª¤è·å–æ—¥å¿—ï¼š
    1. æŸ¥è¯¢ä»»åŠ¡å°è¯•ä¿¡æ¯è·å–å®¹å™¨ ID å’ŒèŠ‚ç‚¹åœ°å€
    2. æŸ¥è¯¢ä½œä¸šä¿¡æ¯è·å–ç”¨æˆ·å
    3. æ„é€ æ—¥å¿— URL å¹¶è·å–æ—¥å¿—å†…å®¹
    
    æ”¯æŒçš„æ—¥å¿—ç±»å‹åŒ…æ‹¬ï¼š
    - stdout: æ ‡å‡†è¾“å‡ºï¼ˆé»˜è®¤ï¼‰
    - stderr: æ ‡å‡†é”™è¯¯
    - syslog: ç³»ç»Ÿæ—¥å¿—
    - syslog.shuffle: Shuffle ç³»ç»Ÿæ—¥å¿—
    - prelaunch.out: é¢„å¯åŠ¨è¾“å‡º
    - prelaunch.err: é¢„å¯åŠ¨é”™è¯¯
    - container-localizer-syslog: å®¹å™¨æœ¬åœ°åŒ–ç³»ç»Ÿæ—¥å¿—
    
    Args:
        params (GetTaskAttemptLogsInput): åŒ…å« job_id, task_id, attempt_id å’Œ log_type
    
    Returns:
        str: æ—¥å¿—å†…å®¹ï¼ŒMarkdown æˆ– JSON æ ¼å¼
        
    Examples:
        - è·å– stdout æ—¥å¿—: job_id="job_xxx", task_id="task_xxx", attempt_id="attempt_xxx"
        - è·å– stderr æ—¥å¿—: job_id="job_xxx", task_id="task_xxx", attempt_id="attempt_xxx", log_type="stderr"
    """
    try:
        # 1. è·å–ä»»åŠ¡å°è¯•ä¿¡æ¯
        attempt_data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks/{params.task_id}/attempts/{params.attempt_id}"
        )
        attempt = attempt_data.get("taskAttempt", {})
        
        container_id = attempt.get("assignedContainerId")
        node_http_address = attempt.get("nodeHttpAddress")
        
        if not container_id:
            return "é”™è¯¯ï¼šæ— æ³•è·å–å®¹å™¨ ID ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ attempt_id æ˜¯å¦æ­£ç¡®ã€‚"
        if not node_http_address:
            return "é”™è¯¯ï¼šæ— æ³•è·å–èŠ‚ç‚¹åœ°å€ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ attempt_id æ˜¯å¦æ­£ç¡®ã€‚"
        
        # 2. è·å–ä½œä¸šä¿¡æ¯ä»¥è·å–ç”¨æˆ·å
        job_data = await _make_request(f"mapreduce/jobs/{params.job_id}")
        job = job_data.get("job", {})
        user = job.get("user")
        
        if not user:
            return "é”™è¯¯ï¼šæ— æ³•è·å–ä½œä¸šç”¨æˆ·ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ job_id æ˜¯å¦æ­£ç¡®ã€‚"
        
        # 3. æ„é€  NodeManager åœ°å€
        hostname = _extract_hostname(node_http_address)
        node_manager = f"{hostname}:{NODEMANAGER_PORT}"
        
        # 4. æ„é€ æ—¥å¿— URL
        log_url = (
            f"{LOGS_BASE_URL}/{node_manager}/{container_id}/"
            f"{params.attempt_id}/{user}/{params.log_type.value}/"
            f"?start=0&start.time=0&end.time=9223372036854775807"
        )
        
        logger.info(f"è·å–æ—¥å¿— URL: {log_url}")
        
        # 5. è·å–æ—¥å¿— HTML
        html_content = await _fetch_logs_html(log_url)
        
        # 6. æå– <pre> æ ‡ç­¾ä¸­çš„æ—¥å¿—å†…å®¹
        log_content = _extract_pre_content(html_content)
        
        if not log_content:
            return f"æ—¥å¿—ä¸ºç©ºæˆ–æ— æ³•è§£ææ—¥å¿—å†…å®¹ã€‚\n\n**æ—¥å¿— URL**: {log_url}"
        
        # 7. æ ¼å¼åŒ–è¾“å‡º
        if params.response_format == ResponseFormat.JSON:
            return json.dumps({
                "job_id": params.job_id,
                "task_id": params.task_id,
                "attempt_id": params.attempt_id,
                "container_id": container_id,
                "node_manager": node_manager,
                "user": user,
                "log_type": params.log_type.value,
                "log_url": log_url,
                "content": log_content
            }, indent=2, ensure_ascii=False)
        
        # Markdown æ ¼å¼è¾“å‡º
        lines = [
            f"# ä»»åŠ¡å°è¯•æ—¥å¿—: {params.log_type.value}",
            "",
            "## æ—¥å¿—ä¿¡æ¯",
            f"| å±æ€§ | å€¼ |",
            f"|------|-----|",
            f"| ä½œä¸š ID | `{params.job_id}` |",
            f"| ä»»åŠ¡ ID | `{params.task_id}` |",
            f"| å°è¯• ID | `{params.attempt_id}` |",
            f"| å®¹å™¨ ID | `{container_id}` |",
            f"| èŠ‚ç‚¹ | {node_manager} |",
            f"| ç”¨æˆ· | {user} |",
            f"| æ—¥å¿—ç±»å‹ | {params.log_type.value} |",
            "",
            "## æ—¥å¿—å†…å®¹",
            "```",
            log_content,
            "```"
        ]
        
        return "\n".join(lines)
        
    except Exception as e:
        return _handle_error(e)


@mcp.tool(
    name="jobhistory_get_task_attempt_logs_partial",
    annotations={
        "title": "éƒ¨åˆ†è¯»å–ä»»åŠ¡å°è¯•æ—¥å¿—",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
@log_tool_call
async def jobhistory_get_task_attempt_logs_partial(params: GetTaskAttemptLogsPartialInput) -> str:
    """
    éƒ¨åˆ†è¯»å–æŒ‡å®šä»»åŠ¡å°è¯•çš„å®¹å™¨æ—¥å¿—å†…å®¹ã€‚
    
    è¯¥å·¥å…·æŒ‰å­—èŠ‚èŒƒå›´è¯»å–æ—¥å¿—ï¼Œé€‚ç”¨äºï¼š
    - å¤§ä»»åŠ¡æˆ–é•¿æœŸè¿è¡Œä»»åŠ¡äº§ç”Ÿçš„å¤§é‡æ—¥å¿—
    - å¿«é€ŸæŸ¥çœ‹æ—¥å¿—æœ«å°¾çš„é”™è¯¯ä¿¡æ¯ï¼ˆä»»åŠ¡å¤±è´¥åˆ†æï¼‰
    - èŠ‚çœ Token æ¶ˆè€—ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–å…¨éƒ¨å†…å®¹
    
    å­—èŠ‚èŒƒå›´å‚æ•°è¯´æ˜ï¼š
    - start: èµ·å§‹å­—èŠ‚ä½ç½®
      - æ­£æ•°: ä»æ–‡ä»¶å¼€å¤´è®¡ç®—ï¼ˆ0 è¡¨ç¤ºç¬¬ä¸€ä¸ªå­—èŠ‚ï¼‰
      - è´Ÿæ•°: ä»æ–‡ä»¶æœ«å°¾å€’æ•°ï¼ˆ-4096 è¡¨ç¤ºå€’æ•° 4096 å­—èŠ‚ï¼‰
    - end: ç»“æŸå­—èŠ‚ä½ç½®
      - æ­£æ•°: å…·ä½“å­—èŠ‚ä½ç½®
      - 0: è¡¨ç¤ºæ–‡ä»¶æœ«å°¾
    
    å¸¸ç”¨åœºæ™¯ï¼š
    - ä»»åŠ¡å¤±è´¥åˆ†æ: start=-4096, end=0 (è¯»å–æœ«å°¾ 4KBï¼Œé€šå¸¸åŒ…å«é”™è¯¯ä¿¡æ¯)
    - æŸ¥çœ‹å¯åŠ¨æ—¥å¿—: start=0, end=2048 (è¯»å–å¼€å¤´ 2KB)
    - è¯»å–ä¸­é—´éƒ¨åˆ†: start=10240, end=20480 (è¯»å– 10KB-20KB èŒƒå›´)
    
    å¦‚æœéƒ¨åˆ†æ—¥å¿—æ— æ³•å®Œæˆåˆ†æï¼Œè¯·ä½¿ç”¨ jobhistory_get_task_attempt_logs è·å–å®Œæ•´æ—¥å¿—ã€‚
    
    Args:
        params (GetTaskAttemptLogsPartialInput): åŒ…å« job_id, task_id, attempt_id, log_type, start, end
    
    Returns:
        str: éƒ¨åˆ†æ—¥å¿—å†…å®¹ï¼ŒMarkdown æˆ– JSON æ ¼å¼
        
    Examples:
        - è¯»å– syslog æœ«å°¾ 4KB: job_id="job_xxx", task_id="task_xxx", attempt_id="attempt_xxx", log_type="syslog"
        - è¯»å– stderr æœ«å°¾ 8KB: job_id="job_xxx", ..., log_type="stderr", start=-8192, end=0
        - è¯»å– stdout å¼€å¤´ 2KB: job_id="job_xxx", ..., log_type="stdout", start=0, end=2048
    """
    try:
        # 1. è·å–ä»»åŠ¡å°è¯•ä¿¡æ¯
        attempt_data = await _make_request(
            f"mapreduce/jobs/{params.job_id}/tasks/{params.task_id}/attempts/{params.attempt_id}"
        )
        attempt = attempt_data.get("taskAttempt", {})
        
        container_id = attempt.get("assignedContainerId")
        node_http_address = attempt.get("nodeHttpAddress")
        
        if not container_id:
            return "é”™è¯¯ï¼šæ— æ³•è·å–å®¹å™¨ ID ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ attempt_id æ˜¯å¦æ­£ç¡®ã€‚"
        if not node_http_address:
            return "é”™è¯¯ï¼šæ— æ³•è·å–èŠ‚ç‚¹åœ°å€ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ attempt_id æ˜¯å¦æ­£ç¡®ã€‚"
        
        # 2. è·å–ä½œä¸šä¿¡æ¯ä»¥è·å–ç”¨æˆ·å
        job_data = await _make_request(f"mapreduce/jobs/{params.job_id}")
        job = job_data.get("job", {})
        user = job.get("user")
        
        if not user:
            return "é”™è¯¯ï¼šæ— æ³•è·å–ä½œä¸šç”¨æˆ·ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ job_id æ˜¯å¦æ­£ç¡®ã€‚"
        
        # 3. æ„é€  NodeManager åœ°å€
        hostname = _extract_hostname(node_http_address)
        node_manager = f"{hostname}:{NODEMANAGER_PORT}"
        
        # 4. æ„é€ æ—¥å¿— URLï¼ˆä½¿ç”¨ start å’Œ end å‚æ•°ï¼‰
        log_url = (
            f"{LOGS_BASE_URL}/{node_manager}/{container_id}/"
            f"{params.attempt_id}/{user}/{params.log_type.value}/"
            f"?start={params.start}&end={params.end}"
        )
        
        logger.info(f"è·å–éƒ¨åˆ†æ—¥å¿— URL: {log_url}")
        
        # 5. è·å–æ—¥å¿— HTML
        html_content = await _fetch_logs_html(log_url)
        
        # 6. æå– <pre> æ ‡ç­¾ä¸­çš„æ—¥å¿—å†…å®¹
        log_content = _extract_pre_content(html_content)
        
        if not log_content:
            return f"æ—¥å¿—ä¸ºç©ºæˆ–æ— æ³•è§£ææ—¥å¿—å†…å®¹ã€‚\n\n**æ—¥å¿— URL**: {log_url}"
        
        # è®¡ç®—è¯»å–èŒƒå›´æè¿°
        if params.start < 0:
            range_desc = f"æœ«å°¾ {abs(params.start)} å­—èŠ‚"
        elif params.end == 0:
            range_desc = f"ä» {params.start} å­—èŠ‚åˆ°æœ«å°¾"
        else:
            range_desc = f"{params.start} - {params.end} å­—èŠ‚"
        
        # 7. æ ¼å¼åŒ–è¾“å‡º
        if params.response_format == ResponseFormat.JSON:
            return json.dumps({
                "job_id": params.job_id,
                "task_id": params.task_id,
                "attempt_id": params.attempt_id,
                "container_id": container_id,
                "node_manager": node_manager,
                "user": user,
                "log_type": params.log_type.value,
                "byte_range": {
                    "start": params.start,
                    "end": params.end,
                    "description": range_desc
                },
                "log_url": log_url,
                "content_length": len(log_content),
                "content": log_content
            }, indent=2, ensure_ascii=False)
        
        # Markdown æ ¼å¼è¾“å‡º
        lines = [
            f"# ä»»åŠ¡å°è¯•æ—¥å¿—ï¼ˆéƒ¨åˆ†ï¼‰: {params.log_type.value}",
            "",
            "## æ—¥å¿—ä¿¡æ¯",
            f"| å±æ€§ | å€¼ |",
            f"|------|-----|",
            f"| ä½œä¸š ID | `{params.job_id}` |",
            f"| ä»»åŠ¡ ID | `{params.task_id}` |",
            f"| å°è¯• ID | `{params.attempt_id}` |",
            f"| å®¹å™¨ ID | `{container_id}` |",
            f"| èŠ‚ç‚¹ | {node_manager} |",
            f"| ç”¨æˆ· | {user} |",
            f"| æ—¥å¿—ç±»å‹ | {params.log_type.value} |",
            f"| è¯»å–èŒƒå›´ | {range_desc} |",
            f"| å†…å®¹é•¿åº¦ | {len(log_content)} å­—èŠ‚ |",
            "",
            "## æ—¥å¿—å†…å®¹",
            "```",
            log_content,
            "```",
            "",
            f"*æç¤ºï¼šå¦‚éœ€å®Œæ•´æ—¥å¿—ï¼Œè¯·ä½¿ç”¨ `jobhistory_get_task_attempt_logs` å·¥å…·*"
        ]
        
        return "\n".join(lines)
        
    except Exception as e:
        return _handle_error(e)


# ==============================================================================
# ä¸»å…¥å£
# ==============================================================================

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ HTTP ä¼ è¾“æ¨¡å¼
    use_http = "--http" in sys.argv or os.getenv("MCP_TRANSPORT", "").lower() == "http"
    
    if use_http:
        # HTTP æ¨¡å¼ - ç”¨äºè¿œç¨‹æœåŠ¡å™¨éƒ¨ç½²
        host = os.getenv("MCP_HOST", "0.0.0.0")
        port = int(os.getenv("MCP_PORT", "8080"))
        logger.info(f"å¯åŠ¨ HTTP ä¼ è¾“æ¨¡å¼: http://{host}:{port}")
        mcp.run(transport="streamable-http", host=host, port=port)
    else:
        # stdio æ¨¡å¼ - é»˜è®¤ï¼Œç”¨äºæœ¬åœ°éƒ¨ç½²
        logger.info("å¯åŠ¨ stdio ä¼ è¾“æ¨¡å¼")
        mcp.run()
